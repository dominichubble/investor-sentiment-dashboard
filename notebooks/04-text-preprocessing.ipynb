{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f78dfc55",
   "metadata": {},
   "source": [
    "# Text Preprocessing for Financial Sentiment Analysis\n",
    "\n",
    "This notebook demonstrates text preprocessing techniques for financial text data collected from Reddit, Twitter, and News APIs. We'll perform:\n",
    "\n",
    "1. **Tokenization** - Split text into individual words/tokens\n",
    "2. **Stopword Removal** - Remove common words while preserving financial terms\n",
    "3. **Lemmatization** - Reduce words to their base form\n",
    "\n",
    "**Why Preprocessing?**\n",
    "- Reduces noise in text data\n",
    "- Standardizes text for sentiment analysis\n",
    "- Reduces vocabulary size for better model performance\n",
    "- Preserves important financial terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43280aa",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import NLTK for natural language processing, pandas for data handling, and our custom preprocessing module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7ed7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Add backend to path for importing our modules\n",
    "backend_path = str(Path.cwd().parent / \"backend\")\n",
    "if backend_path not in sys.path:\n",
    "    sys.path.insert(0, backend_path)\n",
    "\n",
    "# Import our preprocessing module\n",
    "from app.preprocessing import TextProcessor, preprocess_text, tokenize, remove_stopwords, lemmatize_tokens, normalize_text  # type: ignore\n",
    "\n",
    "# Import NLTK for demos\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Display settings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a50853",
   "metadata": {},
   "source": [
    "## 2. Sample Financial Text Data\n",
    "\n",
    "Let's create sample texts from each data source to demonstrate preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba3fa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample texts from different sources\n",
    "sample_texts = {\n",
    "    \"reddit\": \"The stock market is so bullish right now! ðŸš€ $TSLA to the moon! Check out https://reddit.com/r/wallstreetbets #stocks #investing\",\n",
    "    \"twitter\": \"@elonmusk Tesla earnings are AMAZING! The stock is up 15% today ðŸ“ˆ $TSLA #bullish #stocks https://t.co/abc123\",\n",
    "    \"news\": \"Market Analysis: The S&P 500 gained 2.3% today as investors remained optimistic about earnings. Technology stocks led the rally with strong performance.\"\n",
    "}\n",
    "\n",
    "# Display samples\n",
    "for source, text in sample_texts.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{source.upper()} Sample:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(text)\n",
    "    print(f\"\\nLength: {len(text)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb089232",
   "metadata": {},
   "source": [
    "## 3. Text Normalization\n",
    "\n",
    "First step: normalize text by removing URLs, mentions, hashtags, and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f076d3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate normalization\n",
    "reddit_text = sample_texts[\"reddit\"]\n",
    "print(\"ORIGINAL:\")\n",
    "print(reddit_text)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NORMALIZED:\")\n",
    "normalized = normalize_text(reddit_text, lowercase=True, remove_urls=True, expand_hashtags=True)\n",
    "print(normalized)\n",
    "print(\"\\nâœ“ URLs, hashtags, and emojis removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f82b5d5",
   "metadata": {},
   "source": [
    "## 4. Tokenization\n",
    "\n",
    "Split normalized text into individual words (tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c509a454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the normalized text\n",
    "tokens = tokenize(normalized)\n",
    "print(f\"Tokens ({len(tokens)} total):\")\n",
    "print(tokens)\n",
    "print(f\"\\nâœ“ Text split into {len(tokens)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5fcdfa",
   "metadata": {},
   "source": [
    "## 5. Stopword Removal\n",
    "\n",
    "Remove common words while **preserving financial terminology**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c345f537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords while preserving financial terms\n",
    "filtered_tokens = remove_stopwords(tokens, preserve_financial=True)\n",
    "\n",
    "print(f\"Before: {len(tokens)} tokens\")\n",
    "print(tokens)\n",
    "print(f\"\\nAfter: {len(filtered_tokens)} tokens\")\n",
    "print(filtered_tokens)\n",
    "print(f\"\\nâœ“ Removed {len(tokens) - len(filtered_tokens)} stopwords\")\n",
    "print(\"âœ“ Financial terms like 'stock', 'market', 'bullish' preserved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03008114",
   "metadata": {},
   "source": [
    "## 6. Lemmatization\n",
    "\n",
    "Reduce words to their base/dictionary form (e.g., \"stocks\" â†’ \"stock\", \"running\" â†’ \"run\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1da758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize tokens\n",
    "lemmatized_tokens = lemmatize_tokens(filtered_tokens)\n",
    "\n",
    "print(\"Before lemmatization:\")\n",
    "print(filtered_tokens)\n",
    "print(\"\\nAfter lemmatization:\")\n",
    "print(lemmatized_tokens)\n",
    "print(\"\\nâœ“ Words reduced to base form\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683770c8",
   "metadata": {},
   "source": [
    "## 7. Complete Pipeline\n",
    "\n",
    "Use the `preprocess_text()` function to apply all steps at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23bbe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all sample texts at once\n",
    "print(\"=\"*60)\n",
    "print(\"COMPLETE PREPROCESSING PIPELINE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for source, text in sample_texts.items():\n",
    "    print(f\"\\n{source.upper()}:\")\n",
    "    print(f\"Original ({len(text)} chars): {text[:80]}...\")\n",
    "    \n",
    "    # Full preprocessing\n",
    "    processed = preprocess_text(\n",
    "        text,\n",
    "        remove_stopwords_flag=True,\n",
    "        lemmatize=True,\n",
    "        preserve_financial=True,\n",
    "        return_string=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Processed ({len(processed)} chars): {processed}\")\n",
    "    print(f\"Reduction: {100 - (len(processed)/len(text)*100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55b9211",
   "metadata": {},
   "source": [
    "## 8. Using TextProcessor Class\n",
    "\n",
    "Create a reusable processor with custom configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7438bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create processor with custom configuration\n",
    "processor = TextProcessor(\n",
    "    lowercase=True,\n",
    "    remove_urls=True,\n",
    "    remove_stopwords=True,\n",
    "    lemmatize=True,\n",
    "    preserve_financial=True\n",
    ")\n",
    "\n",
    "# Process multiple texts\n",
    "texts = [\n",
    "    \"Tesla stock surged 20% after earnings beat expectations!\",\n",
    "    \"The Federal Reserve announced interest rate cuts today.\",\n",
    "    \"Bitcoin reached new all-time highs as investors remain bullish.\"\n",
    "]\n",
    "\n",
    "print(\"Batch Processing:\")\n",
    "print(\"=\"*60)\n",
    "results = processor.process_batch(texts, return_strings=True)\n",
    "for i, (original, processed) in enumerate(zip(texts, results), 1):\n",
    "    print(f\"\\n{i}. Original: {original}\")\n",
    "    print(f\"   Processed: {processed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943c78b3",
   "metadata": {},
   "source": [
    "## 9. Comparison of Preprocessing Configurations\n",
    "\n",
    "Compare different preprocessing levels: minimal, standard, and full."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927a9966",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"The stock markets are experiencing significant gains today! ðŸ“ˆ https://example.com\"\n",
    "\n",
    "configs = {\n",
    "    \"Minimal (normalize only)\": {\n",
    "        \"remove_stopwords_flag\": False,\n",
    "        \"lemmatize\": False\n",
    "    },\n",
    "    \"Standard (+ stopwords)\": {\n",
    "        \"remove_stopwords_flag\": True,\n",
    "        \"lemmatize\": False,\n",
    "        \"preserve_financial\": True\n",
    "    },\n",
    "    \"Full (+ lemmatization)\": {\n",
    "        \"remove_stopwords_flag\": True,\n",
    "        \"lemmatize\": True,\n",
    "        \"preserve_financial\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"ORIGINAL TEXT:\")\n",
    "print(test_text)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "for config_name, config_params in configs.items():\n",
    "    result = preprocess_text(test_text, **config_params, return_string=True)\n",
    "    tokens = preprocess_text(test_text, **config_params, return_string=False)\n",
    "    print(f\"\\n{config_name}:\")\n",
    "    print(f\"  Result: {result}\")\n",
    "    print(f\"  Tokens: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c7fe8a",
   "metadata": {},
   "source": [
    "## 10. Financial Terms Preservation\n",
    "\n",
    "Demonstrate how financial terminology is preserved even with stopword removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19340195",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.preprocessing.text_processor import FINANCIAL_TERMS  # type: ignore\n",
    "\n",
    "print(\"Financial terms preserved during stopword removal:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total financial terms: {len(FINANCIAL_TERMS)}\")\n",
    "print(f\"\\nSample terms: {sorted(list(FINANCIAL_TERMS))[:20]}\")\n",
    "\n",
    "# Demonstrate preservation\n",
    "financial_text = \"The stock market shows bullish gains with strong returns on investment\"\n",
    "print(f\"\\n\\nOriginal: {financial_text}\")\n",
    "\n",
    "# Without preserving financial terms\n",
    "result_no_preserve = preprocess_text(\n",
    "    financial_text, \n",
    "    remove_stopwords_flag=True, \n",
    "    preserve_financial=False,\n",
    "    return_string=True\n",
    ")\n",
    "print(f\"Without preservation: {result_no_preserve}\")\n",
    "\n",
    "# With preserving financial terms\n",
    "result_preserve = preprocess_text(\n",
    "    financial_text,\n",
    "    remove_stopwords_flag=True,\n",
    "    preserve_financial=True,\n",
    "    return_string=True\n",
    ")\n",
    "print(f\"With preservation: {result_preserve}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8095720",
   "metadata": {},
   "source": [
    "## 11. Save Preprocessed Sample Data\n",
    "\n",
    "Export preprocessed samples for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46c6bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = Path(\"data/preprocessed/samples\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Process and save samples\n",
    "output_data = {\n",
    "    \"metadata\": {\n",
    "        \"processed_at\": datetime.now().isoformat(),\n",
    "        \"configuration\": {\n",
    "            \"lowercase\": True,\n",
    "            \"remove_stopwords\": True,\n",
    "            \"lemmatize\": True,\n",
    "            \"preserve_financial\": True\n",
    "        }\n",
    "    },\n",
    "    \"samples\": []\n",
    "}\n",
    "\n",
    "for source, text in sample_texts.items():\n",
    "    tokens = preprocess_text(\n",
    "        text,\n",
    "        remove_stopwords_flag=True,\n",
    "        lemmatize=True,\n",
    "        preserve_financial=True,\n",
    "        return_string=False\n",
    "    )\n",
    "    \n",
    "    output_data[\"samples\"].append({\n",
    "        \"source\": source,\n",
    "        \"original\": text,\n",
    "        \"processed\": \" \".join(tokens),\n",
    "        \"tokens\": tokens,\n",
    "        \"token_count\": len(tokens)\n",
    "    })\n",
    "\n",
    "# Save to JSON\n",
    "output_file = output_dir / f\"preprocessed_samples_{datetime.now().strftime('%Y%m%d')}.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ“ Saved preprocessed samples to: {output_file}\")\n",
    "print(f\"âœ“ Total samples: {len(output_data['samples'])}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

