{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2d463a9",
   "metadata": {},
   "source": [
    "# Twitter/X Data Ingestion Pipeline\n",
    "\n",
    "This notebook collects finance-related tweets using the Twitter API (via Tweepy).\n",
    "\n",
    "**Features:**\n",
    "- Search tweets by keywords\n",
    "- Filter by language, date range\n",
    "- Clean and normalize tweet text\n",
    "- Export to CSV format\n",
    "\n",
    "**Note:** This is an exploration notebook. For production use, see `backend/app/pipelines/ingest_twitter.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b990cb0",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f66a49ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Tweepy version: 4.16.0\n",
      "‚úì Imports complete\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "# Load environment variables\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  python-dotenv not installed. Install with: pip install python-dotenv\")\n",
    "\n",
    "# Twitter API library\n",
    "try:\n",
    "    import tweepy\n",
    "    print(f\"‚úì Tweepy version: {tweepy.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå Tweepy not installed. Install with: pip install tweepy\")\n",
    "\n",
    "print(\"‚úì Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0fc778",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4a7d2b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: ..\\data\\processed\\twitter\\2025-10-28\n",
      "Keywords: stock market, stocks, earnings, fed rate, inflation, NVDA, TSLA, AAPL, wall street, bull market, bear market\n",
      "Max tweets: 30\n",
      "Min engagement threshold: 5\n"
     ]
    }
   ],
   "source": [
    "# API Credentials (from .env file)\n",
    "TWITTER_BEARER_TOKEN = os.getenv('TWITTER_BEARER_TOKEN')\n",
    "\n",
    "# Search configuration\n",
    "# Note: Free tier = 1,500 tweets/month (50 tweets/day budget)\n",
    "KEYWORDS = [\n",
    "    'stock market',\n",
    "    'stocks',\n",
    "    'earnings',\n",
    "    'fed rate',\n",
    "    'inflation',\n",
    "    'NVDA',\n",
    "    'TSLA',\n",
    "    'AAPL',\n",
    "    'wall street',\n",
    "    'bull market',\n",
    "    'bear market'\n",
    "]\n",
    "\n",
    "MAX_TWEETS = 30  # Conservative: ~50 tweets/day budget for free tier\n",
    "LANGUAGE = 'en'\n",
    "\n",
    "# Quality filters\n",
    "MIN_ENGAGEMENT = 5  # Minimum total engagement (likes + retweets + replies)\n",
    "\n",
    "# Output configuration\n",
    "OUTPUT_DIR = Path('../data/processed/twitter')\n",
    "RUN_ID = datetime.utcnow().strftime('%Y-%m-%d')\n",
    "\n",
    "print(f\"Output directory: {OUTPUT_DIR / RUN_ID}\")\n",
    "print(f\"Keywords: {', '.join(KEYWORDS)}\")\n",
    "print(f\"Max tweets: {MAX_TWEETS}\")\n",
    "print(f\"Min engagement threshold: {MIN_ENGAGEMENT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e249ec9b",
   "metadata": {},
   "source": [
    "## 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "58d6596d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Check out $TSLA! üöÄ @elonmusk https://example.com #stocks #trading\n",
      "Cleaned:  Check out $TSLA! üöÄ stocks trading\n"
     ]
    }
   ],
   "source": [
    "# URL pattern for cleaning\n",
    "_URL_RE = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "\n",
    "def clean_text(txt: Optional[str]) -> str:\n",
    "    \"\"\"\n",
    "    Remove URLs, mentions, hashtags (optional), and normalize whitespace.\n",
    "    \n",
    "    Args:\n",
    "        txt: Input text string\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned text\n",
    "    \"\"\"\n",
    "    if not txt:\n",
    "        return ''\n",
    "    \n",
    "    # Remove URLs\n",
    "    txt = _URL_RE.sub('', txt)\n",
    "    \n",
    "    # Remove @mentions\n",
    "    txt = re.sub(r'@\\w+', '', txt)\n",
    "    \n",
    "    # Keep hashtags but remove # symbol (they're useful for sentiment)\n",
    "    txt = re.sub(r'#(\\w+)', r'\\1', txt)\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    txt = re.sub(r'\\s+', ' ', txt)\n",
    "    \n",
    "    return txt.strip()\n",
    "\n",
    "# Test the function\n",
    "test_tweet = \"Check out $TSLA! üöÄ @elonmusk https://example.com #stocks #trading\"\n",
    "print(f\"Original: {test_tweet}\")\n",
    "print(f\"Cleaned:  {clean_text(test_tweet)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "18c7b5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "def normalize_tweet(tweet) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract and normalize fields from a Twitter API v2 tweet object.\n",
    "    \n",
    "    Args:\n",
    "        tweet: Tweepy Tweet object\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with normalized tweet data\n",
    "    \"\"\"\n",
    "    # Extract user data if available\n",
    "    author_id = tweet.author_id if hasattr(tweet, 'author_id') else None\n",
    "    \n",
    "    # Get metrics\n",
    "    metrics = tweet.public_metrics if hasattr(tweet, 'public_metrics') else {}\n",
    "    \n",
    "    return {\n",
    "        'id': tweet.id,\n",
    "        'text': clean_text(tweet.text),\n",
    "        'raw_text': tweet.text,  # Keep original for reference\n",
    "        'author_id': author_id,\n",
    "        'created_at': tweet.created_at.isoformat() if hasattr(tweet, 'created_at') else None,\n",
    "        'retweet_count': metrics.get('retweet_count', 0),\n",
    "        'reply_count': metrics.get('reply_count', 0),\n",
    "        'like_count': metrics.get('like_count', 0),\n",
    "        'quote_count': metrics.get('quote_count', 0),\n",
    "        'lang': tweet.lang if hasattr(tweet, 'lang') else None,\n",
    "    }\n",
    "\n",
    "print(\"‚úì Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cf447698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: (\"stock market\" OR stocks OR earnings) lang:en -is:retweet\n"
     ]
    }
   ],
   "source": [
    "def build_query(keywords: List[str], lang: str = 'en') -> str:\n",
    "    \"\"\"\n",
    "    Build a Twitter search query from keywords.\n",
    "    \n",
    "    Args:\n",
    "        keywords: List of keywords to search for\n",
    "        lang: Language code (default: 'en')\n",
    "        \n",
    "    Returns:\n",
    "        Query string\n",
    "    \"\"\"\n",
    "    # Quote multi-word phrases\n",
    "    terms = [f'\"{k}\"' if ' ' in k else k for k in keywords]\n",
    "    \n",
    "    # Join with OR and add language filter\n",
    "    query = '(' + ' OR '.join(terms) + f') lang:{lang}'\n",
    "    \n",
    "    # Exclude retweets for cleaner data\n",
    "    query += ' -is:retweet'\n",
    "    \n",
    "    return query\n",
    "\n",
    "# Test the query builder\n",
    "test_query = build_query(KEYWORDS[:3], LANGUAGE)\n",
    "print(f\"Query: {test_query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52be121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Quality filter functions defined\n"
     ]
    }
   ],
   "source": [
    "def filter_spam_and_bots(tweets: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Remove spam, bots, and low-quality tweets.\n",
    "    \n",
    "    Args:\n",
    "        tweets: List of tweet dictionaries\n",
    "        \n",
    "    Returns:\n",
    "        Filtered list of quality tweets\n",
    "    \"\"\"\n",
    "    filtered = []\n",
    "    \n",
    "    # Enhanced spam indicators\n",
    "    spam_patterns = [\n",
    "        # Promotional spam\n",
    "        r'check\\s+(out|latest)',\n",
    "        r'follow\\s+(me|us|for|him)',\n",
    "        r'click\\s+(here|link)',\n",
    "        r'dm\\s+me',\n",
    "        \n",
    "        # Stock promotion spam (CAUGHT THE EXAMPLES!)\n",
    "        r'this\\s+(blogger|investor|trader).+recommends?\\s+stocks?',\n",
    "        r'recommends?\\s+stocks?\\s+that\\s+rise',\n",
    "        r'his\\s+judgment\\s+is.+(accurate|incredible)',\n",
    "        r'buy\\s+the\\s+stocks?\\s+(he|she|they)\\s+recommends?',\n",
    "        r'make\\s+money\\s+every\\s+day',\n",
    "        r'you\\s+can\\s+also\\s+follow',\n",
    "        \n",
    "        # Other spam\n",
    "        r'recommend\\s+a\\s+blogger',\n",
    "        r'just\\s+earned',\n",
    "        r'simulation\\s+market',\n",
    "        r'airdrop',\n",
    "        r'free\\s+money',\n",
    "        r'guaranteed\\s+profit',\n",
    "        r'\\d+%\\s+movement\\s+in',\n",
    "    ]\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        text_lower = tweet['text'].lower()\n",
    "        raw_lower = tweet['raw_text'].lower()\n",
    "        \n",
    "        # Skip if spam pattern detected\n",
    "        is_spam = any(re.search(pattern, text_lower) for pattern in spam_patterns)\n",
    "        if is_spam:\n",
    "            continue\n",
    "        \n",
    "        # Skip if too short (likely not meaningful)\n",
    "        if len(tweet['text']) < 20:\n",
    "            continue\n",
    "        \n",
    "        # Detect suspicious uniform engagement (bot networks)\n",
    "        # Real tweets rarely have EXACTLY the same likes, retweets, and replies\n",
    "        if (tweet['like_count'] == tweet['retweet_count'] == tweet['reply_count'] \n",
    "            and tweet['like_count'] > 0):\n",
    "            continue  # Likely bot network with fake engagement\n",
    "        \n",
    "        # Skip if too many emojis (often spam)\n",
    "        emoji_pattern = r'[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F1E0-\\U0001F1FF]'\n",
    "        emoji_count = len(re.findall(emoji_pattern, tweet['raw_text']))\n",
    "        if emoji_count > 5:  # More than 5 emojis = likely spam\n",
    "            continue\n",
    "        \n",
    "        # Skip if excessive hashtags (common in spam)\n",
    "        hashtag_count = tweet['raw_text'].count('#')\n",
    "        if hashtag_count > 8:  # More than 8 hashtags = spam\n",
    "            continue\n",
    "        \n",
    "        # Skip if contains too many cashtags (spam pattern)\n",
    "        cashtag_count = tweet['raw_text'].count('$')\n",
    "        if cashtag_count > 5:  # More than 5 stock symbols = spam\n",
    "            continue\n",
    "        \n",
    "        # Skip if starts with multiple random emojis (spam signature)\n",
    "        if re.match(r'^[\\U0001F000-\\U0001FFFF\\s]{10,}', tweet['raw_text']):\n",
    "            continue\n",
    "        \n",
    "        filtered.append(tweet)\n",
    "    \n",
    "    return filtered\n",
    "\n",
    "\n",
    "def filter_by_engagement(tweets: List[Dict[str, Any]], min_engagement: int = 5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Keep only tweets with minimum engagement threshold.\n",
    "    \n",
    "    Args:\n",
    "        tweets: List of tweet dictionaries\n",
    "        min_engagement: Minimum total engagement (likes + retweets + replies)\n",
    "        \n",
    "    Returns:\n",
    "        Filtered list of tweets\n",
    "    \"\"\"\n",
    "    return [\n",
    "        t for t in tweets \n",
    "        if (t['like_count'] + t['retweet_count'] + t['reply_count']) >= min_engagement\n",
    "    ]\n",
    "\n",
    "\n",
    "print(\"‚úì Quality filter functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5d7dff",
   "metadata": {},
   "source": [
    "## 4. Initialize Twitter API Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "141d0b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Twitter client initialized\n"
     ]
    }
   ],
   "source": [
    "def initialize_twitter_client() -> tweepy.Client:\n",
    "    \"\"\"\n",
    "    Initialize and authenticate Twitter API client.\n",
    "    \n",
    "    Returns:\n",
    "        Authenticated Tweepy Client\n",
    "    \"\"\"\n",
    "    if not TWITTER_BEARER_TOKEN:\n",
    "        raise ValueError(\n",
    "            'Missing Twitter credentials. Set TWITTER_BEARER_TOKEN '\n",
    "            'environment variable or create a .env file.'\n",
    "        )\n",
    "    \n",
    "    client = tweepy.Client(\n",
    "        bearer_token=TWITTER_BEARER_TOKEN,\n",
    "        wait_on_rate_limit=True  # Automatically handle rate limits\n",
    "    )\n",
    "    \n",
    "    print(\"‚úì Twitter client initialized\")\n",
    "    return client\n",
    "\n",
    "# Initialize client\n",
    "try:\n",
    "    client = initialize_twitter_client()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    client = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876f560d",
   "metadata": {},
   "source": [
    "## 5. Fetch Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fdb4dbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: (\"stock market\" OR stocks OR earnings OR \"fed rate\" OR inflation OR NVDA OR TSLA OR AAPL OR \"wall street\" OR \"bull market\" OR \"bear market\") lang:en -is:retweet\n",
      "Fetching up to 30 tweets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 102 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Fetched 30 raw tweets\n",
      "Applying quality filters...\n",
      "  After spam filter: 25 tweets\n",
      "  After engagement filter (min=5): 2 tweets\n",
      "\n",
      "‚úì Total quality tweets collected: 2\n"
     ]
    }
   ],
   "source": [
    "def fetch_tweets(\n",
    "    client: tweepy.Client,\n",
    "    keywords: List[str],\n",
    "    max_results: int = 100,\n",
    "    lang: str = 'en',\n",
    "    min_engagement: int = 0\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Fetch tweets matching keywords with quality filtering.\n",
    "    \n",
    "    Args:\n",
    "        client: Authenticated Tweepy client\n",
    "        keywords: List of keywords to search for\n",
    "        max_results: Maximum number of tweets to fetch (10-100 per request)\n",
    "        lang: Language code\n",
    "        min_engagement: Minimum engagement threshold\n",
    "        \n",
    "    Returns:\n",
    "        List of normalized tweet dictionaries\n",
    "    \"\"\"\n",
    "    query = build_query(keywords, lang)\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Fetching up to {max_results} tweets...\")\n",
    "    \n",
    "    tweets = []\n",
    "    seen = set()\n",
    "    \n",
    "    try:\n",
    "        # Twitter API v2 recent search\n",
    "        response = client.search_recent_tweets(\n",
    "            query=query,\n",
    "            max_results=min(max_results, 100),  # API limit is 100 per request\n",
    "            tweet_fields=['created_at', 'author_id', 'lang', 'public_metrics'],\n",
    "        )\n",
    "        \n",
    "        # Check if response has data (Pylance may show warning, but this is correct)\n",
    "        if not response.data:  # type: ignore\n",
    "            print(\"‚ö†Ô∏è  No tweets found\")\n",
    "            return []\n",
    "        \n",
    "        for tweet in response.data:  # type: ignore\n",
    "            if tweet.id not in seen:\n",
    "                seen.add(tweet.id)\n",
    "                tweets.append(normalize_tweet(tweet))\n",
    "        \n",
    "        print(f\"‚úì Fetched {len(tweets)} raw tweets\")\n",
    "        \n",
    "        # Apply quality filters\n",
    "        print(\"Applying quality filters...\")\n",
    "        filtered_spam = filter_spam_and_bots(tweets)\n",
    "        print(f\"  After spam filter: {len(filtered_spam)} tweets\")\n",
    "        \n",
    "        filtered_engagement = filter_by_engagement(filtered_spam, min_engagement)\n",
    "        print(f\"  After engagement filter (min={min_engagement}): {len(filtered_engagement)} tweets\")\n",
    "        \n",
    "        return filtered_engagement\n",
    "        \n",
    "    except tweepy.TweepyException as e:\n",
    "        print(f\"‚ùå Error fetching tweets: {e}\")\n",
    "        return []\n",
    "\n",
    "# Fetch tweets with quality filtering\n",
    "if client:\n",
    "    tweets = fetch_tweets(client, KEYWORDS, MAX_TWEETS, LANGUAGE, MIN_ENGAGEMENT)\n",
    "    print(f\"\\n‚úì Total quality tweets collected: {len(tweets)}\")\n",
    "else:\n",
    "    print(\"‚ùå Client not initialized. Cannot fetch tweets.\")\n",
    "    tweets = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1714bf20",
   "metadata": {},
   "source": [
    "## 6. Preview Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b6d933b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 3 tweets:\n",
      "\n",
      "1. ü•™üòãüßΩüåÑ This blogger, . recommends stocks that rise every day. His judgment is incredibly accurate. You...\n",
      "   Likes: 9, Retweets: 9\n",
      "   Created: 2025-10-28T10:16:47+00:00\n",
      "\n",
      "2. ‚èØü•§üêπ‚ôê This investor is so accurate, Buy the stocks he recommends and you will make money every day. Y...\n",
      "   Likes: 9, Retweets: 9\n",
      "   Created: 2025-10-28T10:16:17+00:00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display first few tweets\n",
    "if tweets:\n",
    "    print(f\"\\nFirst 3 tweets:\\n\")\n",
    "    for i, tweet in enumerate(tweets[:3], 1):\n",
    "        print(f\"{i}. {tweet['text'][:100]}...\")\n",
    "        print(f\"   Likes: {tweet['like_count']}, Retweets: {tweet['retweet_count']}\")\n",
    "        print(f\"   Created: {tweet['created_at']}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b6b1fe",
   "metadata": {},
   "source": [
    "## 7. Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "064cce5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Exported 2 tweets to ..\\data\\processed\\twitter\\2025-10-28\\twitter_finance_2025-10-28.csv\n",
      "‚úì Saved metadata to ..\\data\\processed\\twitter\\2025-10-28\\twitter_finance_2025-10-28_meta.txt\n",
      "\n",
      "‚úì Pipeline complete!\n"
     ]
    }
   ],
   "source": [
    "def export_to_csv(\n",
    "    tweets: List[Dict[str, Any]],\n",
    "    output_dir: Path,\n",
    "    run_id: str\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Export tweets to CSV file.\n",
    "    \n",
    "    Args:\n",
    "        tweets: List of tweet dictionaries\n",
    "        output_dir: Output directory path\n",
    "        run_id: Run identifier\n",
    "        \n",
    "    Returns:\n",
    "        Path to output CSV file\n",
    "    \"\"\"\n",
    "    if not tweets:\n",
    "        print(\"‚ö†Ô∏è  No tweets to export\")\n",
    "        return \"\"\n",
    "    \n",
    "    # Create output directory\n",
    "    run_dir = output_dir / run_id\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Output file path\n",
    "    output_file = run_dir / f'twitter_finance_{run_id}.csv'\n",
    "    \n",
    "    # Write to CSV\n",
    "    fieldnames = [\n",
    "        'id', 'text', 'raw_text', 'author_id', 'created_at',\n",
    "        'retweet_count', 'reply_count', 'like_count', 'quote_count', 'lang'\n",
    "    ]\n",
    "    \n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(tweets)\n",
    "    \n",
    "    print(f\"‚úì Exported {len(tweets)} tweets to {output_file}\")\n",
    "    \n",
    "    # Also save metadata\n",
    "    meta_file = run_dir / f'twitter_finance_{run_id}_meta.txt'\n",
    "    with open(meta_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"Run ID: {run_id}\\n\")\n",
    "        f.write(f\"Timestamp: {datetime.utcnow().isoformat()}\\n\")\n",
    "        f.write(f\"Keywords: {', '.join(KEYWORDS)}\\n\")\n",
    "        f.write(f\"Language: {LANGUAGE}\\n\")\n",
    "        f.write(f\"Total tweets: {len(tweets)}\\n\")\n",
    "    \n",
    "    print(f\"‚úì Saved metadata to {meta_file}\")\n",
    "    \n",
    "    return str(output_file)\n",
    "\n",
    "# Export data\n",
    "if tweets:\n",
    "    output_path = export_to_csv(tweets, OUTPUT_DIR, RUN_ID)\n",
    "    print(f\"\\n‚úì Pipeline complete!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No data to export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a23356a",
   "metadata": {},
   "source": [
    "## 8. Data Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4808665d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Summary Statistics:\n",
      "Total tweets: 2\n",
      "Total likes: 18\n",
      "Total retweets: 18\n",
      "Average likes per tweet: 9.0\n",
      "Average retweets per tweet: 9.0\n",
      "\n",
      "Most liked tweet (9 likes):\n",
      "  ü•™üòãüßΩüåÑ This blogger, . recommends stocks that rise every day. His judgment is incredibly accurate. You can also follow him. Premarket rise $GGAL Portfol...\n"
     ]
    }
   ],
   "source": [
    "# Display statistics\n",
    "if tweets:\n",
    "    total_likes = sum(t['like_count'] for t in tweets)\n",
    "    total_retweets = sum(t['retweet_count'] for t in tweets)\n",
    "    avg_likes = total_likes / len(tweets)\n",
    "    avg_retweets = total_retweets / len(tweets)\n",
    "    \n",
    "    print(\"\\nüìä Summary Statistics:\")\n",
    "    print(f\"Total tweets: {len(tweets)}\")\n",
    "    print(f\"Total likes: {total_likes:,}\")\n",
    "    print(f\"Total retweets: {total_retweets:,}\")\n",
    "    print(f\"Average likes per tweet: {avg_likes:.1f}\")\n",
    "    print(f\"Average retweets per tweet: {avg_retweets:.1f}\")\n",
    "    \n",
    "    # Most engaged tweet\n",
    "    most_liked = max(tweets, key=lambda t: t['like_count'])\n",
    "    print(f\"\\nMost liked tweet ({most_liked['like_count']} likes):\")\n",
    "    print(f\"  {most_liked['text'][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298831c0",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Review the collected data** in the CSV file\n",
    "2. **Adjust keywords** if needed for better coverage\n",
    "3. **Convert to production script** once satisfied with results\n",
    "4. **Add to pipeline** alongside Reddit ingestion\n",
    "\n",
    "See `backend/app/pipelines/ingest_twitter.py` for the production version."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
