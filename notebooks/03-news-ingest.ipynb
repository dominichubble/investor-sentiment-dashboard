{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "362ec7e8",
   "metadata": {},
   "source": [
    "# News API Data Ingestion Pipeline\n",
    "\n",
    "This notebook collects finance-related news articles using NewsAPI.org.\n",
    "\n",
    "**Features:**\n",
    "- Search articles by keywords\n",
    "- Filter by sources (Bloomberg, Reuters, WSJ, etc.)\n",
    "- Clean and normalize article text\n",
    "- Export to JSON format\n",
    "\n",
    "**Note:** This is an exploration notebook. For production use, see `backend/app/pipelines/ingest_news.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf94b423",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b83ad5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì newsapi-python installed\n",
      "‚úì Imports complete\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "# Load environment variables\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  python-dotenv not installed. Install with: pip install python-dotenv\")\n",
    "\n",
    "# NewsAPI library\n",
    "try:\n",
    "    from newsapi import NewsApiClient\n",
    "    print(\"‚úì newsapi-python installed\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå newsapi-python not installed. Install with: pip install newsapi-python\")\n",
    "\n",
    "print(\"‚úì Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9debd5",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e50f35ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: ..\\data\\processed\\news\\2025-11-02\n",
      "Keywords: stock market, stocks, earnings, federal reserve, interest rates... (12 total)\n",
      "Sources: bloomberg, reuters, the-wall-street-journal... (7 total)\n",
      "Max articles: 100\n",
      "Date range: Last 7 days\n"
     ]
    }
   ],
   "source": [
    "# API Credentials (from .env file)\n",
    "NEWS_API_KEY = os.getenv('NEWS_API_KEY')\n",
    "\n",
    "# Search configuration\n",
    "# Note: Free tier = 100 requests/day\n",
    "KEYWORDS = [\n",
    "    'stock market',\n",
    "    'stocks',\n",
    "    'earnings',\n",
    "    'federal reserve',\n",
    "    'interest rates',\n",
    "    'inflation',\n",
    "    'NVDA',\n",
    "    'TSLA',\n",
    "    'AAPL',\n",
    "    'market crash',\n",
    "    'bull market',\n",
    "    'bear market'\n",
    "]\n",
    "\n",
    "# Financial news sources (NewsAPI source IDs)\n",
    "SOURCES = [\n",
    "    'bloomberg',\n",
    "    'reuters',\n",
    "    'the-wall-street-journal',\n",
    "    'financial-times',\n",
    "    'business-insider',\n",
    "    'fortune',\n",
    "    'cnbc'\n",
    "]\n",
    "\n",
    "MAX_ARTICLES = 100  # Per request (API limit)\n",
    "LANGUAGE = 'en'\n",
    "DAYS_BACK = 7  # Search last 7 days\n",
    "\n",
    "# Output configuration\n",
    "OUTPUT_DIR = Path('../data/processed/news')\n",
    "RUN_ID = datetime.utcnow().strftime('%Y-%m-%d')\n",
    "\n",
    "print(f\"Output directory: {OUTPUT_DIR / RUN_ID}\")\n",
    "print(f\"Keywords: {', '.join(KEYWORDS[:5])}... ({len(KEYWORDS)} total)\")\n",
    "print(f\"Sources: {', '.join(SOURCES[:3])}... ({len(SOURCES)} total)\")\n",
    "print(f\"Max articles: {MAX_ARTICLES}\")\n",
    "print(f\"Date range: Last {DAYS_BACK} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e55c2a",
   "metadata": {},
   "source": [
    "## 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36d4f670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: <p>NVDA earnings beat! <a href='http://example.com'>Read more</a> [+1234 chars]</p>\n",
      "Cleaned:  NVDA earnings beat! Read more\n"
     ]
    }
   ],
   "source": [
    "# URL pattern for cleaning\n",
    "_URL_RE = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "\n",
    "def clean_text(txt: Optional[str]) -> str:\n",
    "    \"\"\"\n",
    "    Remove HTML tags, URLs, and normalize whitespace.\n",
    "    \n",
    "    Args:\n",
    "        txt: Input text string\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned text\n",
    "    \"\"\"\n",
    "    if not txt:\n",
    "        return ''\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    txt = re.sub(r'<[^>]+>', '', txt)\n",
    "    \n",
    "    # Remove URLs\n",
    "    txt = _URL_RE.sub('', txt)\n",
    "    \n",
    "    # Remove [+XXX chars] artifacts from NewsAPI\n",
    "    txt = re.sub(r'\\[\\+\\d+ chars\\]', '', txt)\n",
    "    \n",
    "    # Remove [Removed] markers\n",
    "    txt = re.sub(r'\\[Removed\\]', '', txt, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    txt = re.sub(r'\\s+', ' ', txt)\n",
    "    \n",
    "    return txt.strip()\n",
    "\n",
    "# Test the function\n",
    "test_article = \"<p>NVDA earnings beat! <a href='http://example.com'>Read more</a> [+1234 chars]</p>\"\n",
    "print(f\"Original: {test_article}\")\n",
    "print(f\"Cleaned:  {clean_text(test_article)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "966668ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "def normalize_article(article: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract and normalize fields from a NewsAPI article object.\n",
    "    \n",
    "    Args:\n",
    "        article: NewsAPI article dictionary\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with normalized article data\n",
    "    \"\"\"\n",
    "    source = article.get('source', {})\n",
    "    \n",
    "    return {\n",
    "        'source_id': source.get('id'),\n",
    "        'source_name': source.get('name'),\n",
    "        'author': article.get('author'),\n",
    "        'title': article.get('title', ''),\n",
    "        'description': article.get('description', ''),\n",
    "        'url': article.get('url'),\n",
    "        'url_to_image': article.get('urlToImage'),\n",
    "        'published_at': article.get('publishedAt'),\n",
    "        'content': article.get('content', ''),\n",
    "        'clean_title': clean_text(article.get('title', '')),\n",
    "        'clean_description': clean_text(article.get('description', '')),\n",
    "        'clean_content': clean_text(article.get('content', '')),\n",
    "    }\n",
    "\n",
    "print(\"‚úì Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e5b9542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: \"stock market\" OR stocks OR earnings OR \"federal reserve\" OR \"interest rates\"\n"
     ]
    }
   ],
   "source": [
    "def build_query(keywords: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Build a NewsAPI search query from keywords.\n",
    "    \n",
    "    Args:\n",
    "        keywords: List of keywords to search for\n",
    "        \n",
    "    Returns:\n",
    "        Query string\n",
    "    \"\"\"\n",
    "    # Quote multi-word phrases, leave single words unquoted\n",
    "    terms = [f'\"{k}\"' if ' ' in k else k for k in keywords]\n",
    "    \n",
    "    # Join with OR for broad coverage\n",
    "    query = ' OR '.join(terms)\n",
    "    \n",
    "    return query\n",
    "\n",
    "# Test the query builder\n",
    "test_query = build_query(KEYWORDS[:5])\n",
    "print(f\"Query: {test_query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7df8aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Quality filter functions defined\n"
     ]
    }
   ],
   "source": [
    "def filter_quality_articles(articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Remove low-quality, paywalled, or removed articles.\n",
    "    \n",
    "    Args:\n",
    "        articles: List of article dictionaries\n",
    "        \n",
    "    Returns:\n",
    "        Filtered list of quality articles\n",
    "    \"\"\"\n",
    "    filtered = []\n",
    "    \n",
    "    for article in articles:\n",
    "        # Skip if title is missing or too short\n",
    "        if not article['clean_title'] or len(article['clean_title']) < 10:\n",
    "            continue\n",
    "        \n",
    "        # Skip if marked as [Removed]\n",
    "        if '[removed]' in article.get('content', '').lower():\n",
    "            continue\n",
    "        \n",
    "        # Skip if content is too short (likely paywalled)\n",
    "        if article['clean_content'] and len(article['clean_content']) < 100:\n",
    "            continue\n",
    "        \n",
    "        # Skip if no description or content\n",
    "        if not article['clean_description'] and not article['clean_content']:\n",
    "            continue\n",
    "        \n",
    "        filtered.append(article)\n",
    "    \n",
    "    return filtered\n",
    "\n",
    "\n",
    "def deduplicate_articles(articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Remove duplicate articles by URL and similar titles.\n",
    "    \n",
    "    Args:\n",
    "        articles: List of article dictionaries\n",
    "        \n",
    "    Returns:\n",
    "        Deduplicated list of articles\n",
    "    \"\"\"\n",
    "    seen_urls = set()\n",
    "    seen_titles = set()\n",
    "    deduped = []\n",
    "    \n",
    "    for article in articles:\n",
    "        url = article['url']\n",
    "        title = article['clean_title'].lower()\n",
    "        \n",
    "        # Skip if we've seen this URL\n",
    "        if url and url in seen_urls:\n",
    "            continue\n",
    "        \n",
    "        # Skip if we've seen very similar title\n",
    "        if title in seen_titles:\n",
    "            continue\n",
    "        \n",
    "        seen_urls.add(url)\n",
    "        seen_titles.add(title)\n",
    "        deduped.append(article)\n",
    "    \n",
    "    return deduped\n",
    "\n",
    "\n",
    "print(\"‚úì Quality filter functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a244af22",
   "metadata": {},
   "source": [
    "## 4. Initialize NewsAPI Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a8fcb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì NewsAPI client initialized\n"
     ]
    }
   ],
   "source": [
    "def initialize_news_client() -> NewsApiClient:\n",
    "    \"\"\"\n",
    "    Initialize and authenticate NewsAPI client.\n",
    "    \n",
    "    Returns:\n",
    "        Authenticated NewsApiClient\n",
    "    \"\"\"\n",
    "    if not NEWS_API_KEY:\n",
    "        raise ValueError(\n",
    "            'Missing NewsAPI credentials. Set NEWS_API_KEY '\n",
    "            'environment variable or create a .env file.'\n",
    "        )\n",
    "    \n",
    "    client = NewsApiClient(api_key=NEWS_API_KEY)\n",
    "    \n",
    "    print(\"‚úì NewsAPI client initialized\")\n",
    "    return client\n",
    "\n",
    "# Initialize client\n",
    "try:\n",
    "    newsapi = initialize_news_client()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    newsapi = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de64270",
   "metadata": {},
   "source": [
    "## 5. Fetch Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20a44d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search query: \"stock market\" OR stocks OR earnings OR \"federal reserve\" OR \"interest rates\" OR inflation OR NVDA O...\n",
      "Fetching up to 100 articles...\n",
      "Date range: 2025-10-26 to 2025-11-02\n",
      "‚úì Fetched 100 raw articles\n",
      "Applying quality filters...\n",
      "  After quality filter: 100 articles\n",
      "  After deduplication: 100 articles\n",
      "\n",
      "‚úì Total quality articles collected: 100\n",
      "‚úì Fetched 100 raw articles\n",
      "Applying quality filters...\n",
      "  After quality filter: 100 articles\n",
      "  After deduplication: 100 articles\n",
      "\n",
      "‚úì Total quality articles collected: 100\n"
     ]
    }
   ],
   "source": [
    "def fetch_articles(\n",
    "    newsapi: NewsApiClient,\n",
    "    keywords: List[str],\n",
    "    sources: Optional[List[str]] = None,\n",
    "    days_back: int = 7,\n",
    "    max_results: int = 100,\n",
    "    language: str = 'en'\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Fetch articles matching keywords with quality filtering.\n",
    "    \n",
    "    Args:\n",
    "        newsapi: Authenticated NewsApiClient\n",
    "        keywords: List of keywords to search for\n",
    "        sources: Optional list of source IDs to filter by\n",
    "        days_back: Number of days to search back\n",
    "        max_results: Maximum number of articles to fetch\n",
    "        language: Language code\n",
    "        \n",
    "    Returns:\n",
    "        List of normalized article dictionaries\n",
    "    \"\"\"\n",
    "    query = build_query(keywords)\n",
    "    print(f\"Search query: {query[:100]}...\" if len(query) > 100 else f\"Search query: {query}\")\n",
    "    print(f\"Fetching up to {max_results} articles...\")\n",
    "    \n",
    "    # Calculate date range (NewsAPI needs YYYY-MM-DD format)\n",
    "    to_date = datetime.utcnow()\n",
    "    from_date = to_date - timedelta(days=days_back)\n",
    "    \n",
    "    print(f\"Date range: {from_date.date()} to {to_date.date()}\")\n",
    "    \n",
    "    try:\n",
    "        # Call NewsAPI everything endpoint\n",
    "        response = newsapi.get_everything(\n",
    "            q=query,\n",
    "            sources=','.join(sources) if sources else None,\n",
    "            from_param=from_date.strftime('%Y-%m-%d'),\n",
    "            to=to_date.strftime('%Y-%m-%d'),\n",
    "            language=language,\n",
    "            sort_by='publishedAt',\n",
    "            page_size=min(max_results, 100)  # API limit is 100 per page\n",
    "        )\n",
    "        \n",
    "        if response['status'] != 'ok':\n",
    "            print(f\"‚ö†Ô∏è  API returned status: {response['status']}\")\n",
    "            return []\n",
    "        \n",
    "        articles = response.get('articles', [])\n",
    "        print(f\"‚úì Fetched {len(articles)} raw articles\")\n",
    "        \n",
    "        # Normalize articles\n",
    "        normalized = [normalize_article(a) for a in articles]\n",
    "        \n",
    "        # Apply quality filters\n",
    "        print(\"Applying quality filters...\")\n",
    "        filtered = filter_quality_articles(normalized)\n",
    "        print(f\"  After quality filter: {len(filtered)} articles\")\n",
    "        \n",
    "        deduped = deduplicate_articles(filtered)\n",
    "        print(f\"  After deduplication: {len(deduped)} articles\")\n",
    "        \n",
    "        return deduped\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error fetching articles: {e}\")\n",
    "        return []\n",
    "\n",
    "# Fetch articles with quality filtering\n",
    "if newsapi:\n",
    "    articles = fetch_articles(\n",
    "        newsapi, \n",
    "        KEYWORDS, \n",
    "        SOURCES,\n",
    "        DAYS_BACK, \n",
    "        MAX_ARTICLES, \n",
    "        LANGUAGE\n",
    "    )\n",
    "    print(f\"\\n‚úì Total quality articles collected: {len(articles)}\")\n",
    "else:\n",
    "    print(\"‚ùå Client not initialized. Cannot fetch articles.\")\n",
    "    articles = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57498ebb",
   "metadata": {},
   "source": [
    "## 6. Preview Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ea3e78ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 3 articles:\n",
      "\n",
      "1. How an 'accidental banker' is turning this LA-based investment bank into one of ...\n",
      "   Source: Business Insider\n",
      "   Published: 2025-11-01T11:08:01Z\n",
      "   Description: Business Insider spoke with Houlihan Lokey's CEO about how he plans to ride the M&A rebound and the ...\n",
      "\n",
      "2. More than half of Gen Z says they only use cash as ‚Äòa last resort‚Äô and doing so ...\n",
      "   Source: Fortune\n",
      "   Published: 2025-11-01T10:02:00Z\n",
      "   Description: Some Gen Zers are so against using cash they‚Äôll forgo shopping from stores that are cash only....\n",
      "\n",
      "3. How Amazon flipped the script on a challenging week...\n",
      "   Source: Business Insider\n",
      "   Published: 2025-11-01T09:24:02Z\n",
      "   Description: Wall Street loved what it saw from Amazon this week. Yes, despite the layoffs....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display first few articles\n",
    "if articles:\n",
    "    print(f\"\\nFirst 3 articles:\\n\")\n",
    "    for i, article in enumerate(articles[:3], 1):\n",
    "        print(f\"{i}. {article['clean_title'][:80]}...\")\n",
    "        print(f\"   Source: {article['source_name']}\")\n",
    "        print(f\"   Published: {article['published_at']}\")\n",
    "        print(f\"   Description: {article['clean_description'][:100]}...\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8ed72d",
   "metadata": {},
   "source": [
    "## 7. Export to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eaceb3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Exported 100 articles to ..\\data\\processed\\news\\2025-11-02\\news_finance_2025-11-02.json\n",
      "‚úì Saved metadata to ..\\data\\processed\\news\\2025-11-02\\news_finance_2025-11-02_meta.json\n",
      "\n",
      "‚úì Pipeline complete!\n"
     ]
    }
   ],
   "source": [
    "def export_to_json(\n",
    "    articles: List[Dict[str, Any]],\n",
    "    output_dir: Path,\n",
    "    run_id: str\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Export articles to JSON file.\n",
    "    \n",
    "    Args:\n",
    "        articles: List of article dictionaries\n",
    "        output_dir: Output directory path\n",
    "        run_id: Run identifier\n",
    "        \n",
    "    Returns:\n",
    "        Path to output JSON file\n",
    "    \"\"\"\n",
    "    if not articles:\n",
    "        print(\"‚ö†Ô∏è  No articles to export\")\n",
    "        return \"\"\n",
    "    \n",
    "    # Create output directory\n",
    "    run_dir = output_dir / run_id\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Output file path\n",
    "    output_file = run_dir / f'news_finance_{run_id}.json'\n",
    "    \n",
    "    # Write to JSON\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(articles, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"‚úì Exported {len(articles)} articles to {output_file}\")\n",
    "    \n",
    "    # Also save metadata\n",
    "    meta_file = run_dir / f'news_finance_{run_id}_meta.json'\n",
    "    metadata = {\n",
    "        'run_id': run_id,\n",
    "        'timestamp': datetime.utcnow().isoformat(),\n",
    "        'total_articles': len(articles),\n",
    "        'sources': list(set(a['source_name'] for a in articles if a['source_name'])),\n",
    "        'date_range_days': DAYS_BACK,\n",
    "    }\n",
    "    with open(meta_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"‚úì Saved metadata to {meta_file}\")\n",
    "    \n",
    "    return str(output_file)\n",
    "\n",
    "# Export data\n",
    "if articles:\n",
    "    output_path = export_to_json(articles, OUTPUT_DIR, RUN_ID)\n",
    "    print(f\"\\n‚úì Pipeline complete!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No data to export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7609807",
   "metadata": {},
   "source": [
    "## 8. Data Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f0f9269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Summary Statistics:\n",
      "Total articles: 100\n",
      "\n",
      "Articles by source:\n",
      "  Business Insider: 59\n",
      "  Fortune: 33\n",
      "  Bloomberg: 4\n",
      "  The Wall Street Journal: 4\n",
      "\n",
      "Most recent article:\n",
      "  How an 'accidental banker' is turning this LA-based investment bank into one of the biggest deal mac...\n",
      "  Published: 2025-11-01T11:08:01Z\n",
      "  Source: Business Insider\n"
     ]
    }
   ],
   "source": [
    "# Display statistics\n",
    "if articles:\n",
    "    sources_count = {}\n",
    "    for article in articles:\n",
    "        source = article['source_name']\n",
    "        if source:\n",
    "            sources_count[source] = sources_count.get(source, 0) + 1\n",
    "    \n",
    "    print(\"\\nüìä Summary Statistics:\")\n",
    "    print(f\"Total articles: {len(articles)}\")\n",
    "    print(f\"\\nArticles by source:\")\n",
    "    for source, count in sorted(sources_count.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  {source}: {count}\")\n",
    "    \n",
    "    # Most recent article\n",
    "    most_recent = max(articles, key=lambda a: a['published_at'] or '')\n",
    "    print(f\"\\nMost recent article:\")\n",
    "    print(f\"  {most_recent['clean_title'][:100]}...\")\n",
    "    print(f\"  Published: {most_recent['published_at']}\")\n",
    "    print(f\"  Source: {most_recent['source_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96343065",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Review the collected data** in the JSON file\n",
    "2. **Adjust keywords or sources** if needed for better coverage\n",
    "3. **Convert to production script** once satisfied with results\n",
    "4. **Add to pipeline** alongside Reddit and Twitter ingestion\n",
    "\n",
    "See `backend/app/pipelines/ingest_news.py` for the production version."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
